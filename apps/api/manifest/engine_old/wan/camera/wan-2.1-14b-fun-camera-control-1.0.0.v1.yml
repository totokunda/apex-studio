api_version: apex/v1
kind: Model
metadata:
  name: Wan 2.1 14B Fun Camera Control
  version: 1.0.0
  description: Wan 2.1 14B Fun Camera Control is a 14B parameter model for camera
    control.
  tags:
  - wan
  - control
  - 14b
  - '2.1'
spec:
  engine: wan
  model_type: control
  components:
  - type: scheduler
    base: src.scheduler.FlowMatchScheduler
    config:
      shift: 3.0
      sigma_min: 0.0
      extra_one_step: true
  - type: vae
    name: wan/vae
    base: wan
    model_path: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/vae
    config_path: https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/resolve/main/vae/config.json
  - type: text_encoder
    name: wan/text_encoder
    base: UMT5EncoderModel
    model_path: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/text_encoder
    config_path: https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/resolve/main/text_encoder/config.json
  - type: transformer
    base: wan.fun
    model_path: https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera/resolve/main/diffusion_pytorch_model.safetensors
    config_path: https://huggingface.co/Wan-AI/Wan2.1-T2V-14B-Diffusers/resolve/main/transformer/config.json
    config: 
      _class_name: WanFunTransformer3DModel
  - type: helper
    base: clip
    name: wan/clip
    model_path: Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/image_encoder
    preprocessor_path: https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P-Diffusers/resolve/main/image_processor/preprocessor_config.json
    processor_class: CLIPImageProcessor
    model_class: CLIPVisionModel
  - type: helper
    base: camera
  defaults:
    run:
      num_inference_steps: 30
      guidance_scale: 5.0
      return_latents: false
      text_encoder_kwargs: 
        use_attention_mask: true
      attention_kwargs: {}
