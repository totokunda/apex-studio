-r ../requirements.txt
-r ../processors.requirements.txt

rembg[gpu]
triton==3.5.1
xformers==0.0.33.post2
transformer_engine==2.10.0

# SageAttention (Source build covers all archs on Linux if TORCH_CUDA_ARCH_LIST is set)
sageattention @ https://huggingface.co/datasets/totoku/attention/resolve/main/sageattention/universal/sageattention-2.2.0-cp312-cp312-linux_x86_64.whl

# FlashAttention 2 (Source build)
flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl

# FlashAttention 3 (Prebuilt wheels - harmless to include even if GPU doesn't support it)
flash_attn_3 @ https://huggingface.co/alexnasa/flash-attn-3/resolve/main/torch29%2Bcu128/flash_attn_3-3.0.0b1-cp39-abi3-linux_x86_64.whl

# Nunchaku
# Install separately via `python scripts/deps/maybe_install_nunchaku.py --install` (defaults to --no-deps)

onnxruntime-gpu==1.23.2
