# NVIDIA CUDA - Ampere (A100 / sm_80, RTX 30xx / sm_86)
#
# Prereq: install a matching PyTorch CUDA wheel first (cu12x), then:
#   pip install -r requirements/machines/cuda-sm80-ampere.txt
#
-r ../requirements.txt
-r ../processors.requirements.txt

rembg[gpu]

# Triton (NVIDIA CUDA)
triton==3.5.1; sys_platform == "linux"
triton-windows; sys_platform == "win32"

# -------------------------
# Windows CUDA stack (cu126 + torch2.6.0 + Python 3.12)
# -------------------------
# We pin torch to match the prebuilt FlashAttention wheel you provided.
torch==2.6.0; sys_platform == "win32" and python_version == "3.12"
torchvision==0.21.0; sys_platform == "win32" and python_version == "3.12"
torchaudio==2.6.0; sys_platform == "win32" and python_version == "3.12"
flash-attn @ https://huggingface.co/lldacing/flash-attention-windows-wheel/resolve/main/flash_attn-2.7.4%2Bcu126torch2.6.0cxx11abiFALSE-cp312-cp312-win_amd64.whl; sys_platform == "win32" and python_version == "3.12"

# Common CUDA perf libs
xformers==0.0.33.post2; sys_platform == "linux"
transformer_engine==2.10.0; sys_platform == "linux"
apex==0.9.10dev; sys_platform == "linux"

# SageAttention "v1" style (pinned from your previous device_requirements)
sageattention==1.0.6; sys_platform == "linux"

# FlashAttention: often easiest from source; this will compile.
flash-attn @ git+https://github.com/Dao-AILab/flash-attention.git; sys_platform == "linux"


