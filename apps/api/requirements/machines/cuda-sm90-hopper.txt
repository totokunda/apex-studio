# NVIDIA CUDA - Hopper (H100 / sm_90)
#
# Prereq: install a matching PyTorch CUDA wheel first (cu12x), then:
#   pip install -r requirements/machines/cuda-sm90-hopper.txt
#
# Note: this repo's `scripts/install.sh` does an extra Hopper build step:
# it builds FlashAttention normally and then builds the Hopper-specific package
# from `flash-attention/hopper/`. Pip-only installs may work but can be finicky.
#
-r ../requirements.txt
-r ../processors.requirements.txt

rembg[gpu]

triton==3.5.1; sys_platform == "linux"
triton-windows; sys_platform == "win32"

xformers==0.0.33.post2; sys_platform == "linux"
transformer_engine==2.10.0; sys_platform == "linux"
# NVIDIA Apex (NOT the unrelated `apex` PyPI package)
apex @ git+https://github.com/NVIDIA/apex.git; sys_platform == "linux"

# SageAttention: source build recommended for Hopper.
sageattention @ git+https://github.com/thu-ml/SageAttention.git; sys_platform == "linux"

# FlashAttention: source build.
flash-attn @ git+https://github.com/Dao-AILab/flash-attention.git; sys_platform == "linux"


