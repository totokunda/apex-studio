# NVIDIA CUDA - Hopper (H100 / sm_90)
#
# Prereq: install a matching PyTorch CUDA wheel first, then:
#   pip install -r requirements/machines/cuda-sm90-hopper.txt
#
# FlashAttention:
# We install both FlashAttention 3 and 2.
# - FA3: prebuilt wheels (CUDA 12.8 + Torch 2.7.x) via windreamer (Linux + Windows)
# - FA2: Windows wheel via HF; Linux uses source build.
#
-r ../requirements.txt
-r ../processors.requirements.txt

rembg[gpu]

triton==3.5.1; sys_platform == "linux"
triton-windows; sys_platform == "win32"

# -------------------------
# Windows CUDA stack (cu128 + torch2.7.1 + Python 3.12)
# -------------------------
sageattention @ https://github.com/woct0rdho/SageAttention/releases/download/v2.2.0-windows/sageattention-2.2.0+cu128torch2.7.1-cp312-cp312-win_amd64.whl; sys_platform == "win32" and python_version == "3.12"
# FlashAttention 2 (Windows): closest matching prebuilt wheel (cu128 + torch2.7.0)
flash-attn @ https://huggingface.co/lldacing/flash-attention-windows-wheel/resolve/main/flash_attn-2.7.4.post1%2Bcu128torch2.7.0cxx11abiFALSE-cp312-cp312-win_amd64.whl; sys_platform == "win32" and python_version == "3.12"

xformers==0.0.33.post2; sys_platform == "linux"
transformer_engine==2.10.0; sys_platform == "linux"
# NVIDIA Apex (NOT the unrelated `apex` PyPI package)
apex @ git+https://github.com/NVIDIA/apex.git; sys_platform == "linux"

# SageAttention: source build recommended for Hopper.
sageattention @ git+https://github.com/thu-ml/SageAttention.git; sys_platform == "linux"

# FlashAttention 3 wheels (Linux + Windows, CUDA 12.8 + Torch 2.7.0)
--extra-index-url https://download.pytorch.org/whl/cu128
--find-links https://windreamer.github.io/flash-attention3-wheels/cu128_torch270
flash_attn_3

# FlashAttention 2 (Linux): source build (provides `flash_attn` module).
flash-attn @ git+https://github.com/Dao-AILab/flash-attention.git; sys_platform == "linux"

onnxruntime-gpu==1.23.2
