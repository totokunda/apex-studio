import numpy as np
from PIL import Image
import torch
from src.engine.base_engine import BaseEngine
import json
from src.text_encoder.text_encoder import TextEncoder
from loguru import logger


def get_gpu_memory(device=None):
    if not torch.cuda.is_available():
        return 0
    device = device if device is not None else torch.cuda.current_device()
    props = torch.cuda.get_device_properties(device)
    if hasattr(torch.cuda, "get_per_process_memory_fraction"):
        memory_fraction = torch.cuda.get_per_process_memory_fraction()
    else:
        memory_fraction = 1.0
    return props.total_memory * memory_fraction


class HunyuanVideo15Shared(BaseEngine):
    """HunyuanVideo15 Shared Engine Implementation"""

    def __init__(self, yaml_path: str, **kwargs):
        super().__init__(yaml_path, **kwargs)

    @staticmethod
    def is_sparse_attn_supported():
        return "nvidia h" in torch.cuda.get_device_properties(0).name.lower()

    def _rescale_noise_cfg(self, noise_cfg, noise_pred_text, guidance_rescale=0.0):
        """
        Rescale `noise_cfg` according to `guidance_rescale`. Based on findings of [Common Diffusion Noise Schedules and
        Sample Steps are Flawed](https://arxiv.org/pdf/2305.08891.pdf). See Section 3.4
        """
        std_text = noise_pred_text.std(
            dim=list(range(1, noise_pred_text.ndim)), keepdim=True
        )
        std_cfg = noise_cfg.std(dim=list(range(1, noise_cfg.ndim)), keepdim=True)
        # rescale the results from guidance (fixes overexposure)
        noise_pred_rescaled = noise_cfg * (std_text / std_cfg)
        # mix with the original results from guidance by factor guidance_rescale to avoid "plain looking" images
        noise_cfg = (
            guidance_rescale * noise_pred_rescaled + (1 - guidance_rescale) * noise_cfg
        )
        return noise_cfg

    def _resize_and_center_crop(self, image, target_width, target_height):
        if target_height == image.shape[0] and target_width == image.shape[1]:
            return image

        pil_image = Image.fromarray(image)
        original_width, original_height = pil_image.size
        scale_factor = max(
            target_width / original_width, target_height / original_height
        )
        resized_width = int(round(original_width * scale_factor))
        resized_height = int(round(original_height * scale_factor))
        resized_image = pil_image.resize((resized_width, resized_height), Image.LANCZOS)
        left = (resized_width - target_width) / 2
        top = (resized_height - target_height) / 2
        right = (resized_width + target_width) / 2
        bottom = (resized_height + target_height) / 2
        cropped_image = resized_image.crop((left, top, right, bottom))
        return np.array(cropped_image)

    def _get_closest_ratio(
        self, height: float, width: float, ratios: list, buckets: list
    ):
        """
        Get the closest ratio in the buckets.

        Args:
            height (float): video height
            width (float): video width
            ratios (list): video aspect ratio
            buckets (list): buckets generated by `generate_crop_size_list`

        Returns:
            the closest size in the buckets and the corresponding ratio
        """
        aspect_ratio = float(height) / float(width)

        ratios_array = np.array(ratios)
        closest_ratio_id = np.abs(ratios_array - aspect_ratio).argmin()
        closest_size = buckets[closest_ratio_id]
        closest_ratio = ratios_array[closest_ratio_id]

        return closest_size, closest_ratio

    def _generate_crop_size_list(sef, base_size=256, patch_size=16, max_ratio=4.0):
        num_patches = round((base_size / patch_size) ** 2)
        assert max_ratio >= 1.0
        crop_size_list = []
        wp, hp = num_patches, 1
        while wp > 0:
            if max(wp, hp) / min(wp, hp) <= max_ratio:
                crop_size_list.append((wp * patch_size, hp * patch_size))
            if (hp + 1) * wp <= num_patches:
                hp += 1
            else:
                wp -= 1
        return crop_size_list

    def _merge_tensor_by_mask(self, tensor_1, tensor_2, mask, dim):
        assert tensor_1.shape == tensor_2.shape
        # Mask is a 0/1 vector. Choose tensor_2 when the value is 1; otherwise, tensor_1
        masked_indices = torch.nonzero(mask).squeeze(1)
        tmp = tensor_1.clone()
        if dim == 0:
            tmp[masked_indices] = tensor_2[masked_indices]
        elif dim == 1:
            tmp[:, masked_indices] = tensor_2[:, masked_indices]
        elif dim == 2:
            tmp[:, :, masked_indices] = tensor_2[:, :, masked_indices]
        return tmp

    def _add_special_token(
        self,
        text_encoder: TextEncoder,
        add_color,
        add_font,
        color_ann_path,
        font_ann_path,
        multilingual=False,
    ):
        """
        Add special tokens for color and font to tokenizer and text encoder.

        Args:
            tokenizer: Huggingface tokenizer.
            text_encoder: Huggingface T5 encoder.
            add_color (bool): Whether to add color tokens.
            add_font (bool): Whether to add font tokens.
            color_ann_path (str): Path to color annotation JSON.
            font_ann_path (str): Path to font annotation JSON.
            multilingual (bool): Whether to use multilingual font tokens.
        """
        with open(font_ann_path, "r") as f:
            idx_font_dict = json.load(f)
        with open(color_ann_path, "r") as f:
            idx_color_dict = json.load(f)

        if multilingual:
            font_token = [
                f"<{font_code[:2]}-font-{idx_font_dict[font_code]}>"
                for font_code in idx_font_dict
            ]
        else:
            font_token = [f"<font-{i}>" for i in range(len(idx_font_dict))]
        color_token = [f"<color-{i}>" for i in range(len(idx_color_dict))]
        additional_special_tokens = []
        if add_color:
            additional_special_tokens += color_token
        if add_font:
            additional_special_tokens += font_token

        tokenizer = text_encoder.tokenizer

        tokenizer.add_tokens(additional_special_tokens, special_tokens=True)
        if not text_encoder.model_loaded:
            text_encoder.model = text_encoder.load_model()
        # Set mean_resizing=False to avoid PyTorch LAPACK dependency

        if hasattr(text_encoder.model, "resize_token_embeddings"):
            text_encoder.model.resize_token_embeddings(
                len(tokenizer), mean_resizing=False
            )
        else:
            logger.warning(
                "Text encoder model does not support resizing token embeddings."
            )

    @staticmethod
    def get_vae_inference_config(memory_limitation=None):
        if memory_limitation is None:
            memory_limitation = get_gpu_memory()
        GB = 1024 * 1024 * 1024
        if memory_limitation < 23 * GB:
            sample_size = 160
            tile_overlap_factor = 0.2
            dtype = torch.float16
        else:
            sample_size = 256
            tile_overlap_factor = 0.25
            dtype = torch.float32
        return {
            "sample_size": sample_size,
            "tile_overlap_factor": tile_overlap_factor,
            "dtype": dtype,
        }
